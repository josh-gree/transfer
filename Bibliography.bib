@article{Radon1986,
abstract = {When one integrates a function of two variables x,y - a point function f(P) in the plane - subject to suitable regularity conditions along an arbitrary straight line g then one obtains in the integral values F(g), a line function. In Part A of the present paper the problem which is solved is the inversion of this linear functional transformation, that is the following questions are answered: can every line function satisfying suitable regularity conditions be regarded as constructed in this way? If so, is f uniquely known from F and how can f be calculated? In Part B a solution of the dual problem of calculating a line function F(g) from its point mean values f(P) is solved in a certain sense. Finally, in Part C certain generalizations are discussed, prompted by consideration of non-Euclidean manifolds as well as higher dimensional spaces. The treatment of these problems, themselves of interest, gains enhanced importance through the numerous relationships that exist between this topic and the theory of logarithmic and Newtonian potentials. These are mentioned at appropriate places in the text.},
author = {Radon, J},
doi = {10.1109/TMI.1986.4307775},
file = {:Users/joshua/Library/Application Support/Mendeley Desktop/Downloaded/Radon - 1986 - On the Determination of Functions from Their Integral Values along Certain Manifolds(2).pdf:pdf},
isbn = {0278-0062},
issn = {0278-0062},
journal = {IEEE transactions on medical imaging},
number = {4},
pages = {170--176},
pmid = {18244009},
title = {{On the Determination of Functions from Their Integral Values along Certain Manifolds.}},
volume = {5},
year = {1986}
}

@ARTICLE{2016arXiv160502688short,
   author = {{Theano Development Team}},
    title = "{Theano: A {Python} framework for fast computation of mathematical expressions}",
  journal = {arXiv e-prints},
   volume = {abs/1605.02688},
 primaryClass = "cs.SC",
 keywords = {Computer Science - Symbolic Computation, Computer Science - Learning, Computer Science - Mathematical Software},
     year = 2016,
    month = may,
      url = {http://arxiv.org/abs/1605.02688},
}

@article{vanAarle201535,
title = "The \{ASTRA\} Toolbox: A platform for advanced algorithm development in electron tomography ",
journal = "Ultramicroscopy ",
volume = "157",
number = "",
pages = "35 - 47",
year = "2015",
note = "",
issn = "0304-3991",
doi = "http://dx.doi.org/10.1016/j.ultramic.2015.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0304399115001060",
author = "Wim van Aarle and Willem Jan Palenstijn and Jan De Beenhouwer and Thomas Altantzis and Sara Bals and K. Joost Batenburg and Jan Sijbers",
keywords = "Electron tomography",
keywords = "Reconstruction",
keywords = "ASTRA Toolbox",
keywords = "Dual-axis ",
abstract = "Abstract We present the \{ASTRA\} Toolbox as an open platform for 3D image reconstruction in tomography. Most of the software tools that are currently used in electron tomography offer limited flexibility with respect to the geometrical parameters of the acquisition model and the algorithms used for reconstruction. The \{ASTRA\} Toolbox provides an extensive set of fast and flexible building blocks that can be used to develop advanced reconstruction algorithms, effectively removing these limitations. We demonstrate this flexibility, the resulting reconstruction quality, and the computational efficiency of this toolbox by a series of experiments, based on experimental dual-axis tilt series. "
}

@article{Palenstijn2011250,
title = "Performance improvements for iterative electron tomography reconstruction using graphics processing units (GPUs) ",
journal = "Journal of Structural Biology ",
volume = "176",
number = "2",
pages = "250 - 253",
year = "2011",
note = "",
issn = "1047-8477",
doi = "http://dx.doi.org/10.1016/j.jsb.2011.07.017",
url = "http://www.sciencedirect.com/science/article/pii/S1047847711002267",
author = "W.J. Palenstijn and K.J. Batenburg and J. Sijbers",
keywords = "Electron tomography",
keywords = "Reconstruction",
keywords = "GPU ",
abstract = "Iterative reconstruction algorithms are becoming increasingly important in electron tomography of biological samples. These algorithms, however, impose major computational demands. Parallelization must be employed to maintain acceptable running times. Graphics Processing Units (GPUs) have been demonstrated to be highly cost-effective for carrying out these computations with a high degree of parallelism. In a recent paper by Xu et al. (2010), a \{GPU\} implementation strategy was presented that obtains a speedup of an order of magnitude over a previously proposed GPU-based electron tomography implementation. In this technical note, we demonstrate that by making alternative design decisions in the \{GPU\} implementation, an additional speedup can be obtained, again of an order of magnitude. By carefully considering memory access locality when dividing the workload among blocks of threads, the GPU’s cache is used more efficiently, making more effective use of the available memory bandwidth. "
}

@article{Schoonjans2011776,
title = "The xraylib library for X-ray–matter interactions. Recent developments ",
journal = "Spectrochimica Acta Part B: Atomic Spectroscopy ",
volume = "66",
number = "11–12",
pages = "776 - 784",
year = "2011",
note = "",
issn = "0584-8547",
doi = "http://dx.doi.org/10.1016/j.sab.2011.09.011",
url = "http://www.sciencedirect.com/science/article/pii/S0584854711001984",
author = "Tom Schoonjans and Antonio Brunetti and Bruno Golosio and Manuel Sanchez del Rio and Vicente Armando Solé and Claudio Ferrero and Laszlo Vincze",
keywords = "X-ray fluorescence",
keywords = "Quantification",
keywords = "Fundamental parameters",
keywords = "Software library ",
abstract = "This work presents the recent developments of xraylib, an \{ANSI\} C library that provides convenient access to a large number of X-ray related databases, with a focus on quantitative X-ray fluorescence applications. The enhancements include improved X-ray fluorescence production cross sections that take into account cascade effects and M-lines, as well as revised line energies, atomic level widths, Compton broadening profiles etc. A full overview of the complete application programming interface is presented. "
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={2579-2605},
  pages={85},
  year={2008}
}

@article{kheruka2011study,
  title={A study to improve the image quality in low-dose computed tomography (SPECT) using filtration},
  author={Kheruka, SC and Naithani, UC and Maurya, AK and Painuly, NK and Aggarwal, LM and Gambhir, S and others},
  journal={Indian Journal of Nuclear Medicine},
  volume={26},
  number={1},
  pages={14},
  year={2011},
  publisher={Medknow Publications}
}

@article{barrett2004artifacts,
  title={Artifacts in CT: recognition and avoidance 1},
  author={Barrett, Julia F and Keat, Nicholas},
  journal={Radiographics},
  volume={24},
  number={6},
  pages={1679--1691},
  year={2004},
  publisher={Radiological Society of North America}
}

@article{hunter2014characterization,
  title={Characterization and correction of cupping effect artefacts in cone beam CT},
  author={Hunter, AK and McDavid, WD},
  journal={Dentomaxillofacial Radiology},
  year={2014},
  publisher={The British Institute of Radiology. 36 Portland Place, London, W1B 1AT}
}

@article{herman1983comparative,
  title={A comparative study of two postreconstruction beam hardening correction methods},
  author={Herman, Gabor T and Trivedi, Sushma S},
  journal={Medical Imaging, IEEE Transactions on},
  volume={2},
  number={3},
  pages={128--135},
  year={1983},
  publisher={IEEE}
}

@article{alvarez1976energy,
  title={Energy-selective reconstructions in x-ray computerised tomography},
  author={Alvarez, Robert E and Macovski, Albert},
  journal={Physics in medicine and biology},
  volume={21},
  number={5},
  pages={733},
  year={1976},
  publisher={IOP Publishing}
}

@misc{berge2007xcom,
  title={XCOM: photon cross sections database},
  author={Berge, MJ and Hubbell, JH and Seltzer, SM and Chang, J and Coursey, JS and Sukumar, R and Zucker, DS},
  year={2007}
}

@article{hornak2008basics,
  title={The basics of MRI, 2008},
  author={Hornak, Joseph P},
  journal={URL http://www. cis. rit. edu/htbooks/mri/index. html},
  volume={68},
  year={2008}
}

@article{alessio2006pet,
  title={PET image reconstruction},
  author={Alessio, Adam and Kinahan, Paul},
  journal={Nuclear Medicine},
  volume={2},
  year={2006},
  publisher={Elsevier, Philadelphia, Pa, USA,}
}

@inproceedings{nesterov1983method,
  title={A method for unconstrained convex minimization problem with the rate of convergence O (1/k2)},
  author={Nesterov, Yurii},
  booktitle={Doklady an SSSR},
  volume={269},
  number={3},
  pages={543--547},
  year={1983}
}

@article{qian1999momentum,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier}
}

@misc{shewchuk1994introduction,
  title={An introduction to the conjugate gradient method without the agonizing pain},
  author={Shewchuk, Jonathan Richard},
  year={1994},
  publisher={Carnegie-Mellon University. Department of Computer Science}
}

@article{dennis1977quasi,
  title={Quasi-Newton methods, motivation and theory},
  author={Dennis, Jr, John E and Mor{\'e}, Jorge J},
  journal={SIAM review},
  volume={19},
  number={1},
  pages={46--89},
  year={1977},
  publisher={SIAM}
}

@article{gordon1974tutorial,
  title={A tutorial on ART (algebraic reconstruction techniques)},
  author={Gordon, Richard},
  journal={Nuclear Science, IEEE Transactions on},
  volume={21},
  number={3},
  pages={78--93},
  year={1974},
  publisher={IEEE}
}

@article{andersen1984simultaneous,
  title={Simultaneous algebraic reconstruction technique (SART): a superior implementation of the ART algorithm},
  author={Andersen, Anders H and Kak, Avinash C},
  journal={Ultrasonic imaging},
  volume={6},
  number={1},
  pages={81--94},
  year={1984},
  publisher={SAGE Publications}
}

@incollection{van1987numerical,
  title={Numerical solution of large, sparse linear algebraic systems arising from tomographic problems},
  author={Van der Sluis, A and Van der Vorst, HA},
  booktitle={Seismic tomography},
  pages={49--83},
  year={1987},
  publisher={Springer}
}

@phdthesis{rezvani2012iterative,
  title={Iterative reconstruction algorithms for polyenergetic x-ray computerized tomography},
  author={Rezvani, Nargol},
  year={2012},
  school={University of Toronto}
}

@article{ranganathan2004levenberg,
  title={The levenberg-marquardt algorithm},
  author={Ranganathan, Ananth},
  journal={Tutoral on LM algorithm},
  pages={1--5},
  year={2004}
}

@article{fletcher1971modified,
  title={A modified Marquardt subroutine for non-linear least squares},
  author={Fletcher, Roger},
  year={1971},
  publisher={Theoretical Physics Division, Atomic Energy Research Establishment}
}

@article{hager2006survey,
  title={A survey of nonlinear conjugate gradient methods},
  author={Hager, William W and Zhang, Hongchao},
  journal={Pacific journal of Optimization},
  volume={2},
  number={1},
  pages={35--58},
  year={2006}
}

@article{bjorck1994numerics,
  title={Numerics of gram-schmidt orthogonalization},
  author={Bj{\"o}rck, {\AA}ke},
  journal={Linear Algebra and Its Applications},
  volume={197},
  pages={297--316},
  year={1994},
  publisher={Elsevier}
}

@article{zhang2014statistical,
  title={Statistical models and regularization strategies in statistical image reconstruction of low-dose X-ray CT: a survey},
  author={Zhang, Hao and Wang, Jing and Ma, Jianhua and Lu, Hongbing and Liang, Zhengrong},
  journal={arXiv preprint arXiv:1412.1732},
  year={2014}
}

@article{burger2014total,
  title={Total variation regularization in measurement and image space for PET reconstruction},
  author={Burger, Martin and M{\"u}ller, Jahn and Papoutsellis, Evangelos and Sch{\"o}nlieb, Carola-Bibiane},
  journal={Inverse Problems},
  volume={30},
  number={10},
  pages={105003},
  year={2014},
  publisher={IOP Publishing}
}

@inproceedings{jang2012information,
  title={Information theoretic discrepancy based iterative reconstruction for transmission tomography},
  author={Jang, Kwang Eun and Sung, Younghun and Lee, Jongha and Lee, Kangeui and Lee, Jae Hak and Lee, Seongdeok},
  booktitle={Biomedical Imaging (ISBI), 2012 9th IEEE International Symposium on},
  pages={642--645},
  year={2012},
  organization={IEEE}
}

@article{graps1995introduction,
  title={An introduction to wavelets},
  author={Graps, Amara},
  journal={Computational Science \& Engineering, IEEE},
  volume={2},
  number={2},
  pages={50--61},
  year={1995},
  publisher={IEEE}
}

@inproceedings{sodickson2015rapid,
  title={The rapid imaging renaissance: sparser samples, denser dimensions, and glimmerings of a grand unified tomography},
  author={Sodickson, Daniel K and Feng, Li and Knoll, Florian and Cloos, Martijn and Ben-Eliezer, Noam and Axel, Leon and Chandarana, Hersh and Block, Tobias and Otazo, Ricardo},
  booktitle={SPIE Medical Imaging},
  pages={94170G--94170G},
  year={2015},
  organization={International Society for Optics and Photonics}
}

@inproceedings{rath2009comparative,
  title={A comparative study of some greedy pursuit algorithms for sparse approximation},
  author={Rath, Gagan and Sahoo, Arabinda},
  booktitle={Signal Processing Conference, 2009 17th European},
  pages={398--402},
  year={2009},
  organization={IEEE}
}

@article{semerci2014tensor,
  title={Tensor-based formulation and nuclear norm regularization for multienergy computed tomography},
  author={Semerci, Oguz and Hao, Ning and Kilmer, Misha E and Miller, Eric L},
  journal={Image Processing, IEEE Transactions on},
  volume={23},
  number={4},
  pages={1678--1693},
  year={2014},
  publisher={IEEE}
}

@article{gao2011multi,
  title={Multi-energy CT based on a prior rank, intensity and sparsity model (PRISM)},
  author={Gao, Hao and Yu, Hengyong and Osher, Stanley and Wang, Ge},
  journal={Inverse problems},
  volume={27},
  number={11},
  pages={115012},
  year={2011},
  publisher={IOP Publishing}
}

@book{buzug2008computed,
  title={Computed tomography: from photon statistics to modern cone-beam CT},
  author={Buzug, Thorsten M},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{poludniowski2009spekcalc,
  title={SpekCalc: a program to calculate photon spectra from tungsten anode x-ray tubes},
  author={Poludniowski, G and Landry, G and DeBlois, F and Evans, PM and Verhaegen, F},
  journal={Physics in medicine and biology},
  volume={54},
  number={19},
  pages={N433},
  year={2009},
  publisher={IOP Publishing}
}

@article{hounsfield1973computerized,
  title={Computerized transverse axial scanning (tomography): Part 1. Description of system},
  author={Hounsfield, Godfrey N},
  journal={The British journal of radiology},
  volume={46},
  number={552},
  pages={1016--1022},
  year={1973},
  publisher={The British Institute of Radiology}
}

@article{wright1999numerical,
  title={Numerical optimization},
  author={Wright, Stephen and Nocedal, Jorge},
  journal={Springer Science},
  volume={35},
  pages={67--68},
  year={1999}
}

@article{Zhu,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.10593v1},
author = {Zhu, Jun-yan and Park, Taesung and Efros, Alexei A and Ai, Berkeley and Berkeley, U C},
eprint = {arXiv:1703.10593v1},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Unpaired{\_}Image{\_}to{\_}Image{\_}Translation{\_}using{\_}Cycle{\_}Consistent{\_}Adversarial{\_}Networks.pdf:pdf},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}}
}
@article{Greff,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.06228v2},
author = {Greff, Klaus},
eprint = {arXiv:1507.06228v2},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Training{\_}Very{\_}Deep{\_}Networks.pdf:pdf},
pages = {1--11},
title = {{Training Very Deep Networks}}
}
@article{Etworks2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1702.06257v1},
author = {Etworks, C Onvolutional N Eural N and Sandler, Mark and Zhmoginov, Andrey},
eprint = {arXiv:1702.06257v1},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/THE{\_}POWER{\_}OF{\_}SPARSITY{\_}IN{\_}CONVOLUTIONAL{\_}NEURAL{\_}NETWORKS.pdf:pdf},
pages = {1--13},
title = {{T HE P OWER OF S PARSITY IN}},
year = {2017}
}
@article{Jaderberg,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02025v3},
author = {Jaderberg, Max and Deepmind, Google},
eprint = {arXiv:1506.02025v3},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Spatial{\_}Transformer{\_}Networks.pdf:pdf},
pages = {1--15},
title = {{Spatial Transformer Networks}}
}
@article{Hinton,
author = {Hinton, Geoffrey},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/RMSPROP.pdf:pdf},
title = {{Neural Networks for Machine Learning Lecture 6a Overview of mini- ­ ‐ batch gradient descent Reminder : The error surface for a linear neuron}}
}
@article{Zoph2017,
author = {Zoph, Barret and Le, Quoc V},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/NEURAL{\_}ARCHITECTURE{\_}SEARCH{\_}WITH{\_}REINFORCEMENT{\_}LEARNING.pdf:pdf},
title = {{N EURAL A RCHITECTURE S EARCH WITH}},
year = {2017}
}
@article{Chen,
author = {Chen, Hu and Zhang, Yi and Kalra, Mannudeep K and Lin, Feng and Liao, Peixi and Zhou, Jiliu and Wang, Ge},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Low{\_}Dose{\_}CT{\_}with{\_}a{\_}Residual{\_}Encoder{\_}Decoder{\_}Convolutional{\_}Neural{\_}Network{\_}RED{\_}CNN.pdf:pdf},
title = {{Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network ( RED-CNN )}}
}
@article{Isola,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.07004v1},
author = {Isola, Phillip and Efros, Alexei A},
eprint = {arXiv:1611.07004v1},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Image{\_}to{\_}Image{\_}Translation{\_}with{\_}Conditional{\_}Adversarial{\_}Networks.pdf:pdf},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}}
}
@article{Mao,
archivePrefix = {arXiv},
arxivId = {arXiv:1606.08921v3},
author = {Mao, Xiao-jiao and Shen, Chunhua and Yang, Yu-bin},
eprint = {arXiv:1606.08921v3},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Image{\_}Restoration{\_}Using{\_}Convolutional{\_}Auto{\_}encoders{\_}with{\_}Symmetric{\_}Skip{\_}Connections.pdf:pdf},
pages = {1--17},
title = {{Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections}}
}
@article{Greff2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.00387v2},
author = {Greff, Klaus},
eprint = {arXiv:1505.00387v2},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Highway{\_}Networks.pdf:pdf},
title = {{Highway Networks}},
year = {2015}
}
@article{Batenburg2013,
author = {Batenburg, Kees Joost},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Fast{\_}Tomographic{\_}Reconstruction{\_}from{\_}Limited{\_}Data{\_}Using{\_}Artificial{\_}Neural{\_}Networks.pdf:pdf},
number = {12},
pages = {5238--5251},
title = {{Fast Tomographic Reconstruction from Limited}},
volume = {22},
year = {2013}
}
@article{Xu2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.00853v2},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi},
eprint = {arXiv:1505.00853v2},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Empirical{\_}Evaluation{\_}of{\_}Rectified{\_}Activations{\_}in{\_}Convolution{\_}Network.pdf:pdf},
title = {{Network}},
year = {2015}
}
@article{Duchi,
author = {Duchi, John},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Efficient{\_}Learning{\_}using{\_}Forward{\_}Backward{\_}Splitting.pdf:pdf},
title = {{Efficient Learning using Forward-Backward Splitting}}
}
@article{Gregor2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04623v2},
author = {Gregor, Karol and Graves, Alex and Com, Wierstra Google},
eprint = {arXiv:1502.04623v2},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/DRAW{\_}A{\_}Recurrent{\_}Neural{\_}Network{\_}For{\_}Image{\_}Generation.pdf:pdf},
title = {{DRAW : A Recurrent Neural Network For Image Generation arXiv : 1502 . 04623v2 [ cs . CV ] 20 May 2015}},
year = {2014}
}
@article{Jin2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.03679v1},
author = {Jin, Kyong Hwan and Mccann, Michael T and Froustey, Emmanuel and Unser, Michael},
eprint = {arXiv:1611.03679v1},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Deep{\_}Convolutional{\_}Neural{\_}Network{\_}for{\_}Inverse{\_}Problems in{\_}Imaging.pdf:pdf},
pages = {1--20},
title = {{Deep Convolutional Neural Network for Inverse Problems in Imaging}},
year = {2016}
}
@article{Bora,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.03208v1},
author = {Bora, Ashish and Price, Eric and Dimakis, Alexandros G},
eprint = {arXiv:1703.03208v1},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Compressed{\_}Sensing{\_}using{\_}Generative{\_}Models.pdf:pdf},
title = {{Compressed Sensing using Generative Models}}
}
@article{Welling,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6114v10},
author = {Welling, Max},
eprint = {arXiv:1312.6114v10},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Auto{\_}Encoding{\_}Variational{\_}Bayes.pdf:pdf},
number = {Ml},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes arXiv : 1312 . 6114v10 [ stat . ML ] 1 May 2014}}
}
@article{Hohenfellner2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1702.08014v1},
author = {Hohenfellner, Markus and Hadaschik, Boris and Radtke, Jan-philipp},
eprint = {arXiv:1702.08014v1},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Adversarial{\_}Networks{\_}for{\_}the{\_}Detection{\_}of{\_}Aggressive{\_}Prostate{\_}Cancer.pdf:pdf},
title = {{Adversarial Networks for the Detection of Aggressive Prostate Cancer}},
year = {2017}
}
@article{Duchi2011,
author = {Duchi, John},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Adaptive{\_}Subgradient{\_}Methods{\_}for{\_}Online{\_}Learning{\_}and{\_}Stochastic{\_}Optimization.pdf:pdf},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization ∗}},
volume = {12},
year = {2011}
}
@article{Kingma2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v8},
author = {Kingma, Diederik P},
eprint = {arXiv:1412.6980v8},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/ADAM{\_}A{\_}METHOD{\_}FOR{\_}STOCHASTIC{\_}OPTIMIZATION.pdf:pdf},
pages = {1--15},
title = {{A : a m s o}},
year = {2015}
}
@article{There,
archivePrefix = {arXiv},
arxivId = {arXiv:1212.5701v1},
author = {There, Related Work and Rate, Learning and There, Annealing},
eprint = {arXiv:1212.5701v1},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/ADADELTA{\_}AN{\_}ADAPTIVE{\_}LEARNING{\_}RATE{\_}METHOD.pdf:pdf},
title = {{(3) − 1 where H}}
}
@article{Lippman2000,
author = {Lippman, Andrew},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/A{\_}Unifying{\_}View{\_}of{\_}Image{\_}Similarity.pdf:pdf},
number = {September},
pages = {1--5},
title = {{A Unifying View of Image Similarity}},
year = {2000}
}
@article{Dumoulin2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.07285v1},
author = {Dumoulin, Vincent and Visin, Francesco},
eprint = {arXiv:1603.07285v1},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/A{\_}guide{\_}to{\_}convolution{\_}arithmetic{\_}for{\_}deep{\_}learning.pdf:pdf},
pages = {1--28},
title = {{A guide to convolution arithmetic for deep learning}},
year = {2016}
}
@article{,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.09736v2},
eprint = {arXiv:1610.09736v2},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/A{\_}deep{\_}convolutional{\_}neural{\_}network{\_}using{\_}directional{\_}wavelets{\_}for{\_}low{\_}dose{\_}Xray{\_}CT{\_}reconstruction.pdf:pdf},
pages = {1--29},
title = {1 , 2 ,}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/UNSUPERVISED{\_}REPRESENTATION{\_}LEARNING{\_}WITH{\_}DEEP{\_}CONVOLUTIONAL{\_}GENERATIVE{\_}ADVERSARIAL{\_}NETWORKS.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv},
pages = {1--15},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/U{\_}Net{\_}Convolutional{\_}Networks{\_}for{\_}Biomedical{\_}Image{\_}Segmentation.pdf:pdf},
isbn = {978-3-319-24573-7},
issn = {16113349},
journal = {Miccai},
pages = {234--241},
pmid = {23285570},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
year = {2015}
}
@article{Yeh2016,
abstract = {In this paper, we propose a novel method for image inpainting based on a Deep Convolutional Generative Adversarial Network (DCGAN). We define a loss function consisting of two parts: (1) a contextual loss that preserves similarity between the input corrupted image and the recovered image, and (2) a perceptual loss that ensures a perceptually realistic output image. Given a corrupted image with missing values, we use back-propagation on this loss to map the corrupted image to a smaller latent space. The mapped vector is then passed through the generative model to predict the missing content. The proposed framework is evaluated on the CelebA and SVHN datasets for two challenging inpainting tasks with random 80{\%} corruption and large blocky corruption. Experiments show that our method can successfully predict semantic information in the missing region and achieve pixel-level photorealism, which is impossible by almost all existing methods.},
archivePrefix = {arXiv},
arxivId = {1607.07539},
author = {Yeh, Raymond and Chen, Chen and Lim, Teck Yian and Hasegawa-Johnson, Mark and Do, Minh N.},
eprint = {1607.07539},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Semantic{\_}Image{\_}Inpainting{\_}with{\_}Perceptual{\_}and{\_}Contextual{\_}Losses.pdf:pdf},
journal = {Arxiv},
title = {{Semantic Image Inpainting with Perceptual and Contextual Losses}},
url = {http://arxiv.org/abs/1607.07539},
year = {2016}
}
@article{Eigen2013,
abstract = {Photographs taken through a window are often compromised by dirt or $\backslash$nrain present on the window surface. Common cases of this include pictures taken $\backslash$nfrom inside a vehicle, or outdoor security cameras mounted inside a protective $\backslash$nenclosure. At capture time, defocus can be used to remove the artifacts, but $\backslash$nthis relies on achieving a shallow depth-of-field and placement of the camera $\backslash$nclose to the window. Instead, we present a post-capture image processing $\backslash$nsolution that can remove localized rain and dirt artifacts from a single image. $\backslash$nWe collect a dataset of clean/corrupted image pairs which are then used to train $\backslash$na specialized form of convolutional neural network. This learns how to map $\backslash$ncorrupted image patches to clean ones, implicitly capturing the characteristic $\backslash$nappearance of dirt and water droplets in natural images. Our models demonstrate $\backslash$neffective removal of dirt and rain in outdoor test conditions.},
author = {Eigen, David and Krishnan, Dilip and Fergus, Rob},
doi = {10.1109/ICCV.2013.84},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Restoring{\_}An{\_}Image{\_}Taken{\_}Through{\_}a{\_}Window{\_}Covered{\_}with{\_}Dirt{\_}or{\_}Rain.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {633--640},
title = {{Restoring an image taken through a window covered with dirt or rain}},
year = {2013}
}
@article{Jain2009,
abstract = {Abstract We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models. We ... $\backslash$n},
author = {Jain, Viren and Seung, Sebastian},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/natural-image-denoising-with-convolutional-networks.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems},
pages = {769--776},
title = {{Natural Image Denoising with Convolutional Networks}},
url = {http://papers.nips.cc/paper/3506-natural-image-denoising-with-convolutional-networks{\%}5Cnpapers3://publication/uuid/69A9E02B-2622-431D-A237-DBB51D3A3AE1},
volume = {21},
year = {2009}
}
@article{Xie2012,
abstract = {We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method achieves state-of-the-art performance in the image denoising task. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning.},
author = {Xie, Junyuan and Xu, Linli and Chen, Enhong},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/image-denoising-and-inpainting-with-deep-neural-networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{Image Denoising and Inpainting with Deep Neural Networks}},
url = {https://nips.cc/Conferences/2012/Program/event.php?ID=3279},
year = {2012}
}
@article{Zhang,
archivePrefix = {arXiv},
arxivId = {1607.08707},
author = {Zhang, Hanming and Li, Liang and Qiao, Kai and Wang, Linyuan and Yan, Bin and Li, Lei and Hu, Guoen},
eprint = {1607.08707},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Image{\_}Prediction{\_}for{\_}Limited{\_}angle{\_}Tomography{\_}via{\_}Deep{\_}Learning{\_}with{\_}Convolutional{\_}Neural{\_}Network.pdf:pdf},
title = {{Image Prediction for Limited-angle Tomography via Deep Learning with Convolutional Neural Network Abstract :}}
}
@article{Burger2012,
abstract = {Image denoising can be described as the problem of mapping from a noisy image to a noise-free image. The best currently available denoising methods approximate this mapping with cleverly engineered algorithms. In this work we attempt to learn this mapping directly with a plain multi layer perceptron (MLP) applied to image patches. While this has been done before, we will show that by training on large image databases we are able to compete with the current state-of-the-art image denoising methods. Furthermore, our approach is easily adapted to less extensively studied types of noise (by merely exchanging the training data), for which we achieve excellent results as well.},
author = {Burger, Harold C. and Schuler, Christian J. and Harmeling, Stefan},
doi = {10.1109/CVPR.2012.6247952},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Image{\_}denoising{\_}Can{\_}plain{\_}Neural{\_}Networks{\_}compete{\_}with{\_}BM3D?.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2392--2399},
title = {{Image denoising: Can plain neural networks compete with BM3D?}},
year = {2012}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ij and Pouget-Abadie, J and Mirza, Mehdi},
eprint = {arXiv:1406.2661v1},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Generative{\_}Adversarial{\_}Networks.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
title = {{Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Han2016,
abstract = {Recently, compressed sensing (CS) computed tomography (CT) using sparse projection views has been extensively investigated to reduce the potential risk of radiation to patient. However, due to the insufficient number of projection views, an analytic reconstruction approach results in severe streaking artifacts and CS-based iterative approach is computationally very expensive. To address this issue, here we propose a novel deep residual learning approach for sparse view CT reconstruction. Specifically, based on a novel persistent homology analysis showing that the manifold of streaking artifacts is topologically simpler than original ones, a deep residual learning architecture that estimates the streaking artifacts is developed. Once a streaking artifact image is estimated, an artifact-free image can be obtained by subtracting the streaking artifacts from the input image. Using extensive experiments with real patient data set, we confirm that the proposed residual learning provides significantly better image reconstruction performance with several orders of magnitude faster computational speed.},
archivePrefix = {arXiv},
arxivId = {1611.06391},
author = {Han, Yoseop and Yoo, Jaejoon and Ye, Jong Chul},
eprint = {1611.06391},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Deep{\_}Residual{\_}Learning{\_}for{\_}Compressed{\_}Sensing{\_}CT{\_}Reconstruction{\_}via{\_}Persistent{\_}Homology{\_}Analysis.pdf:pdf},
title = {{Deep Residual Learning for Compressed Sensing CT Reconstruction via Persistent Homology Analysis}},
url = {http://arxiv.org/abs/1611.06391},
year = {2016}
}
@article{Mirza2014,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Conditional{\_}Generative{\_}Adversarial{\_}Nets.pdf:pdf},
journal = {CoRR},
pages = {1--7},
title = {{Conditional Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1411.1784},
year = {2014}
}
@article{Zhang2016,
abstract = {Discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise (AWGN) at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks such as Gaussian denoising, single image super-resolution and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.},
archivePrefix = {arXiv},
arxivId = {1608.03981},
author = {Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
eprint = {1608.03981},
file = {:Users/joshua/PHD/convince{\_}thomas{\_}PHD/papers/optimisers/Beyond{\_}a{\_}Gaussian{\_}Denoiser{\_}Residual{\_}Learning{\_}of{\_}Deep{\_}CNN{\_}for{\_}Image{\_}Denoising.pdf:pdf},
pages = {1--13},
title = {{Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising}},
url = {http://arxiv.org/abs/1608.03981},
year = {2016}
}

Prox_tv -> https://github.com/albarji/proxTV
@Article{barberoTV14,
  Title                    = {Modular proximal optimization for multidimensional total-variation regularization},
  Author                   = {\'Alvaro Barbero and Suvrit Sra},
  Year                     = {2014},
  Url                      = {http://arxiv.org/abs/1411.0589}
}

